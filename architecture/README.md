# Architecture Design for "Innovate Inc."

> **Cloud Provider:** Amazon Web Services (AWS)
> **Managed Kubernetes:** Amazon EKS
> **Last Updated:** February 2026

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Cloud Environment Structure](#2-cloud-environment-structure)
3. [High-Level Architecture Diagram](#3-high-level-architecture-diagram)
4. [Network Design](#4-network-design)
5. [Compute Platform (EKS)](#5-compute-platform-eks)
6. [Database Strategy](#6-database-strategy)
7. [Security](#7-security)
8. [CI/CD Pipeline](#8-cicd-pipeline)
9. [Observability & Monitoring](#9-observability--monitoring)
10. [Cost Optimization](#10-cost-optimization)
11. [Scalability Strategy](#11-scalability-strategy)
12. [Disaster Recovery](#12-disaster-recovery)
13. [Technology Decisions Summary](#13-technology-decisions-summary)

---

## 1. Executive Summary

Innovate Inc. is building a web application (Python/Flask API + React SPA + PostgreSQL) that starts with low traffic but must scale to millions of users. This document outlines a **cloud-native, scalable, and secure** architecture on AWS leveraging managed services to minimize operational overhead for a small team.

### Key Design Principles:
- **Start small, scale fast** â€” cost-optimized for low traffic, ready for millions
- **Security first** â€” sensitive user data handled throughout
- **Developer velocity** â€” CI/CD from day one
- **Managed services** â€” minimize undifferentiated heavy lifting
- **Infrastructure as Code** â€” everything reproducible via Terraform

---

## 2. Cloud Environment Structure

### 2.1 Recommended: Multi-Account Strategy (AWS Organizations)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AWS Organizations                  â”‚
â”‚                 (Management Account)                 â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Production  â”‚  â”‚   Staging   â”‚  â”‚    Dev      â”‚ â”‚
â”‚  â”‚   Account    â”‚  â”‚   Account   â”‚  â”‚   Account   â”‚ â”‚
â”‚  â”‚              â”‚  â”‚             â”‚  â”‚             â”‚ â”‚
â”‚  â”‚ Real users   â”‚  â”‚ Pre-prod    â”‚  â”‚ Engineers   â”‚ â”‚
â”‚  â”‚ Real data    â”‚  â”‚ Integration â”‚  â”‚ Experiments â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  Security/  â”‚  â”‚   Shared    â”‚                  â”‚
â”‚  â”‚  Audit Acct â”‚  â”‚  Services   â”‚                  â”‚
â”‚  â”‚             â”‚  â”‚  (CI/CD,    â”‚                  â”‚
â”‚  â”‚ CloudTrail  â”‚  â”‚   ECR,      â”‚                  â”‚
â”‚  â”‚ GuardDuty   â”‚  â”‚   DNS)      â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Account Breakdown

| Account | Purpose | Justification |
|---------|---------|---------------|
| **Management** | AWS Organizations, consolidated billing, SCPs | Single pane of glass for governance |
| **Production** | Live environment, real user data | Strict isolation, limited access |
| **Staging** | Pre-production testing | Mirrors prod, safe testing ground |
| **Development** | Feature development, experiments | Engineers have more freedom |
| **Security/Audit** | CloudTrail logs, GuardDuty, centralized security | Immutable audit trail, no one can delete logs |
| **Shared Services** | CI/CD pipelines, ECR (container registry), Route 53 | Shared resources reduce duplication |

### 2.3 Why Multi-Account?

- **Blast radius isolation**: A security incident in Dev doesn't affect Production
- **Billing clarity**: Per-account cost tracking for each environment
- **IAM boundaries**: No cross-account access without explicit trust policies
- **Compliance**: Sensitive production data never touches development
- **Service quotas**: Each account has its own AWS service limits

### 2.4 Phase-Based Rollout

> **Start simple, evolve gradually:**
>
> **Phase 1 (MVP):** Production + Development accounts only
> **Phase 2 (Growth):** Add Staging + Shared Services
> **Phase 3 (Scale):** Full multi-account structure with Security/Audit

---

## 3. High-Level Architecture Diagram

### 3.1 Overall System Architecture

```
                         INTERNET
                            â”‚
                            â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   CloudFront   â”‚  â† CDN + WAF
                   â”‚   + WAF + ACM  â”‚    (DDoS Protection)
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚                         â”‚
               â–¼                         â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   React SPA    â”‚       â”‚  Application   â”‚
      â”‚  (S3 + CF)     â”‚       â”‚  Load Balancer â”‚
      â”‚  Static Files  â”‚       â”‚  (HTTPS only)  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              VPC (10.0.0.0/16)       â”‚
                    â”‚                                      â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚    Private Subnet (Multi-AZ)   â”‚  â”‚
                    â”‚  â”‚                                â”‚  â”‚
                    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
                    â”‚  â”‚  â”‚      EKS CLUSTER         â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚                          â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”‚Flask   â”‚ â”‚Flask   â”‚  â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”‚Pod     â”‚ â”‚Pod     â”‚  â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”‚(x86)   â”‚ â”‚(ARM64) â”‚  â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚                          â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”‚    Karpenter      â”‚    â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â”‚  (Auto Scaling)   â”‚    â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚  â”‚
                    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â”‚                                      â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚  â”‚    Data Subnet (Multi-AZ)      â”‚  â”‚
                    â”‚  â”‚                                â”‚  â”‚
                    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
                    â”‚  â”‚  â”‚   RDS PostgreSQL          â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚   (Multi-AZ, Encrypted)   â”‚  â”‚  â”‚
                    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
                    â”‚  â”‚                                â”‚  â”‚
                    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
                    â”‚  â”‚  â”‚   ElastiCache Redis       â”‚  â”‚  â”‚
                    â”‚  â”‚  â”‚   (Session/Cache)         â”‚  â”‚  â”‚
                    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 CI/CD Pipeline Flow

```
Developer â”€â”€pushâ”€â”€â–º GitHub
                       â”‚
                       â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   GitHub Actions â”‚
              â”‚                 â”‚
              â”‚  1. Run tests   â”‚
              â”‚  2. Build image â”‚
              â”‚  3. Push to ECR â”‚
              â”‚  4. Update Helm â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                         â”‚
          â–¼                         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Staging   â”‚           â”‚ Production  â”‚
   â”‚   EKS       â”‚  â”€â”€â”€â”€â”€â”€â–º  â”‚   EKS       â”‚
   â”‚  (auto)     â”‚ (manual   â”‚  (manual    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  approve) â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Traffic Flow: User Request Journey

```
User Browser
    â”‚
    â”‚ HTTPS (443)
    â–¼
CloudFront (CDN)
    â”‚
    â”œâ”€â”€[Static assets]â”€â”€â–º S3 (React SPA HTML/JS/CSS)
    â”‚
    â””â”€â”€[/api/* requests]â”€â”€â–º Application Load Balancer
                                      â”‚
                                      â”‚ HTTP (80) internal
                                      â–¼
                             EKS Ingress Controller
                             (AWS Load Balancer Controller)
                                      â”‚
                                      â–¼
                               Flask Service
                               (ClusterIP)
                                      â”‚
                                      â–¼
                               Flask Pods
                               (Auto-scaled by HPA + Karpenter)
                                      â”‚
                                      â–¼
                               RDS PostgreSQL
                               (Private subnet, encrypted)
```

---

## 4. Network Design

### 4.1 VPC Architecture

```
VPC: 10.0.0.0/16
â”‚
â”œâ”€â”€ Public Subnets (Internet-facing, Load Balancers only)
â”‚   â”œâ”€â”€ us-east-1a: 10.0.0.0/24
â”‚   â”œâ”€â”€ us-east-1b: 10.0.1.0/24
â”‚   â””â”€â”€ us-east-1c: 10.0.2.0/24
â”‚
â”œâ”€â”€ Private Subnets (Application layer - EKS nodes)
â”‚   â”œâ”€â”€ us-east-1a: 10.0.10.0/23
â”‚   â”œâ”€â”€ us-east-1b: 10.0.12.0/23
â”‚   â””â”€â”€ us-east-1c: 10.0.14.0/23
â”‚
â””â”€â”€ Data Subnets (Database layer - RDS, ElastiCache)
    â”œâ”€â”€ us-east-1a: 10.0.20.0/24
    â”œâ”€â”€ us-east-1b: 10.0.21.0/24
    â””â”€â”€ us-east-1c: 10.0.22.0/24
```

**Key design decisions:**
- **3-tier subnet model** â€” public/private/data separation
- **3 Availability Zones** â€” high availability and fault tolerance
- **Private subnets for EKS nodes** â€” application servers never directly exposed
- **Data subnets isolated** â€” database only reachable from application layer
- **NAT Gateway per AZ** â€” no single point of failure for outbound traffic

### 4.2 Security Groups

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Security Group Design                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ALB Security Group â”‚ Inbound: 443 from 0.0.0.0/0         â”‚
â”‚                    â”‚ Outbound: 8080 â†’ EKS nodes SG        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ EKS Nodes SG       â”‚ Inbound: 8080 from ALB SG only       â”‚
â”‚                    â”‚ Inbound: 443 from EKS Control Plane  â”‚
â”‚                    â”‚ Outbound: 5432 â†’ RDS SG               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RDS Security Group â”‚ Inbound: 5432 from EKS Nodes SG only â”‚
â”‚                    â”‚ No outbound internet                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ElastiCache SG     â”‚ Inbound: 6379 from EKS Nodes SG only â”‚
â”‚                    â”‚ No outbound internet                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 Network Security Measures

| Layer | Control | Implementation |
|-------|---------|---------------|
| **Edge** | DDoS protection | AWS Shield Standard (free) + CloudFront |
| **Edge** | Web Application Firewall | AWS WAF rules (OWASP Top 10) |
| **DNS** | Private DNS | Route 53 Private Hosted Zones |
| **Transit** | Encryption in transit | TLS 1.2+ enforced everywhere |
| **VPC** | Network ACLs | Stateless packet filtering per subnet |
| **Instance** | Security Groups | Stateful, least-privilege rules |
| **Pod** | Network Policies | Kubernetes NetworkPolicy via Calico/Cilium |
| **API** | Endpoint security | EKS private endpoint (no public API) |

### 4.4 EKS Private Endpoint

```yaml
# EKS API server is private only â€” no public access
eks_endpoint_public_access  = false   # â† No public Kubernetes API
eks_endpoint_private_access = true    # â† Only accessible from within VPC
```

**Access to cluster:** Only via VPN, bastion host, or AWS Systems Manager Session Manager (SSM).

---

## 5. Compute Platform (EKS)

### 5.1 EKS Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EKS CLUSTER                          â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   System Node Group  â”‚  â”‚   Karpenter (Dynamic)   â”‚  â”‚
â”‚  â”‚  (Always-on)        â”‚  â”‚                         â”‚  â”‚
â”‚  â”‚                     â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  3x t3.medium       â”‚  â”‚  â”‚Graviton â”‚ â”‚ x86   â”‚  â”‚  â”‚
â”‚  â”‚  On-Demand          â”‚  â”‚  â”‚ Spot    â”‚ â”‚ Spot  â”‚  â”‚  â”‚
â”‚  â”‚  Multi-AZ           â”‚  â”‚  â”‚ (ARM64) â”‚ â”‚(AMD64)â”‚  â”‚  â”‚
â”‚  â”‚                     â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  Runs:              â”‚  â”‚                         â”‚  â”‚
â”‚  â”‚  - Karpenter        â”‚  â”‚  Scales 0 â†’ âˆ           â”‚  â”‚
â”‚  â”‚  - CoreDNS          â”‚  â”‚  based on demand        â”‚  â”‚
â”‚  â”‚  - AWS LB Ctrl      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚             Application Workloads               â”‚   â”‚
â”‚  â”‚                                                 â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  Flask API       â”‚  â”‚   Background Jobs   â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Deployment      â”‚  â”‚   (Workers)         â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  replicas: 3-50  â”‚  â”‚   replicas: 1-20    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  HPA enabled     â”‚  â”‚   KEDA/HPA          â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Node Groups Strategy

#### System Node Group (Managed, Always-On)
```
Purpose:    Critical cluster infrastructure
Instances:  t3.medium (x86)
Count:      2-3 nodes (min for HA)
Type:       On-Demand (reliability)
Runs:       Karpenter, CoreDNS, Metrics Server, ALB Controller
```

#### Karpenter Node Pools (Dynamic Scaling)

| NodePool | Architecture | Capacity | Use Case |
|----------|-------------|----------|----------|
| `general-arm64` | ARM64 (Graviton) | Spot â†’ On-Demand | Stateless Flask API pods |
| `general-x86` | AMD64 (x86) | Spot â†’ On-Demand | Any x86-required workloads |
| `memory-optimized` | ARM64/x86 | Spot â†’ On-Demand | Data processing, caching |

**Instance Type Examples:**
```
Graviton (ARM64): t4g, m7g, c7g, r7g families
x86 (AMD64):      t3, m7i, c7i, r7i families
```

### 5.3 Application Deployment

#### Flask API Deployment
```yaml
# Simplified representation of the Flask deployment
kind: Deployment
metadata:
  name: flask-api
spec:
  replicas: 3
  template:
    spec:
      # Allow Karpenter to choose best available node
      nodeSelector:
        kubernetes.io/arch: arm64   # Prefer Graviton (cheaper)
      tolerations:
        - key: karpenter.sh/capacity-type
          value: spot               # Accept Spot instances
      containers:
      - name: flask-api
        image: <account>.dkr.ecr.us-east-1.amazonaws.com/innovateinc/api:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            cpu: "250m"
            memory: "256Mi"
          limits:
            cpu: "1000m"
            memory: "512Mi"
        # Health checks
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
        # Secrets via AWS Secrets Manager / K8s Secrets
        env:
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
```

#### Horizontal Pod Autoscaler (HPA)
```yaml
kind: HorizontalPodAutoscaler
metadata:
  name: flask-api-hpa
spec:
  scaleTargetRef:
    name: flask-api
  minReplicas: 3        # Always 3 for HA
  maxReplicas: 50       # Handles millions of users
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70   # Scale at 70% CPU
  - type: Resource
    resource:
      name: memory
      target:
        averageUtilization: 80   # Scale at 80% memory
```

### 5.4 Containerization Strategy

#### Image Building
```
Code Push â†’ GitHub Actions CI
                â”‚
                â–¼
         Build Docker Image
         (Multi-stage build)
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
         â”‚             â”‚
         â–¼             â–¼
      amd64          arm64
     (linux/amd64) (linux/arm64)
         â”‚             â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                â–¼
         Multi-arch manifest
         Push to ECR:
         <acct>.dkr.ecr.us-east-1.amazonaws.com/
           innovateinc/api:latest
           innovateinc/api:v1.2.3
```

#### Dockerfile (Multi-stage, production-optimized)
```dockerfile
# Stage 1: Build
FROM python:3.12-slim AS builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user --no-cache-dir -r requirements.txt

# Stage 2: Runtime (minimal image)
FROM python:3.12-slim
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .

# Run as non-root user (security)
RUN adduser --disabled-password --gecos '' appuser
USER appuser

EXPOSE 8080
CMD ["gunicorn", "--bind", "0.0.0.0:8080", "app:app"]
```

#### Image Security
- **ECR image scanning:** Automatic vulnerability scanning on push
- **Non-root containers:** All containers run as non-root
- **Read-only filesystems:** Where possible
- **Minimal base images:** `python:3.12-slim` not full `python:3.12`
- **Image signing:** Cosign for supply chain integrity

### 5.5 Kubernetes Add-ons

| Add-on | Purpose |
|--------|---------|
| **Karpenter** | Intelligent node auto-scaling |
| **AWS Load Balancer Controller** | Manage ALB/NLB from Kubernetes |
| **External DNS** | Auto-manage Route 53 records |
| **External Secrets Operator** | Sync AWS Secrets Manager â†’ K8s Secrets |
| **Metrics Server** | Enable HPA |
| **AWS EBS CSI Driver** | Persistent volumes |
| **CoreDNS** | Cluster DNS |
| **Calico / Cilium** | Network policies |

---

## 6. Database Strategy

### 6.1 Recommended Service: Amazon RDS for PostgreSQL

**Choice: Amazon RDS PostgreSQL (not Aurora)**

> Initially RDS PostgreSQL. Migrate to **Aurora PostgreSQL Serverless v2** when traffic grows above 10,000+ concurrent users.

#### Phase 1: RDS PostgreSQL (Startup Phase)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RDS PostgreSQL Multi-AZ                â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Primary         â”‚      â”‚  Standby         â”‚    â”‚
â”‚  â”‚  (us-east-1a)    â”‚ â•â•â•â•â–º â”‚  (us-east-1b)    â”‚    â”‚
â”‚  â”‚  db.t3.medium    â”‚      â”‚  db.t3.medium    â”‚    â”‚
â”‚  â”‚  (Read + Write)  â”‚      â”‚  (Failover only) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Read Replica (us-east-1c)                   â”‚   â”‚
â”‚  â”‚  db.t3.medium                                â”‚   â”‚
â”‚  â”‚  (Read-only traffic â€” analytics, reporting)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Justification for RDS over Aurora at startup:**
- Lower cost (no minimum capacity unit charges)
- Simpler operation
- Easier to reason about for a small team
- Easy migration path to Aurora when needed

#### Phase 2: Aurora PostgreSQL Serverless v2 (Growth Phase)

When the application reaches scale:
```
RDS PostgreSQL â†’ Aurora PostgreSQL Serverless v2

Benefits:
- Scales compute automatically (0.5 â†’ 128 ACUs)
- Global Database for multi-region
- Built-in HA (6 copies across 3 AZs)
- Up to 15 read replicas
- No maintenance windows impact
```

### 6.2 Database Configuration

```hcl
# Terraform representation
resource "aws_db_instance" "main" {
  engine         = "postgres"
  engine_version = "16.4"           # Latest PostgreSQL 16
  instance_class = "db.t3.medium"   # Start small

  allocated_storage     = 100       # 100 GB initial
  max_allocated_storage = 1000      # Auto-scale up to 1 TB

  multi_az               = true     # HA failover
  storage_encrypted      = true     # Encrypted at rest (KMS)
  deletion_protection    = true     # Prevent accidents

  backup_retention_period = 35      # 35 days backups
  backup_window           = "03:00-04:00"   # Off-peak hours

  # Performance Insights
  performance_insights_enabled = true
  monitoring_interval          = 60  # Enhanced monitoring

  # No public access
  publicly_accessible = false
  db_subnet_group_name = aws_db_subnet_group.main.name
  vpc_security_group_ids = [aws_security_group.rds.id]
}
```

### 6.3 High Availability

| Scenario | Solution | RTO | RPO |
|----------|----------|-----|-----|
| **AZ failure** | Multi-AZ automatic failover | < 2 min | ~0 |
| **Instance failure** | RDS Multi-AZ standby | < 2 min | ~0 |
| **Region failure** | Cross-region read replica â†’ promote | ~30 min | < 5 min |
| **Data corruption** | Point-in-time recovery (PITR) | ~1 hour | < 5 min |
| **Accidental delete** | Automated snapshots (35 days) | ~1 hour | < 24 hr |

### 6.4 Backup Strategy

```
Backup Type           â”‚ Frequency   â”‚ Retention  â”‚ Storage
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Automated RDS backups â”‚ Daily       â”‚ 35 days    â”‚ S3 (same region)
Manual snapshots      â”‚ Pre-deploy  â”‚ 90 days    â”‚ S3 (same region)
Cross-region copy     â”‚ Daily       â”‚ 30 days    â”‚ S3 (DR region)
Point-in-time (PITR)  â”‚ Continuous  â”‚ 35 days    â”‚ S3 (auto)
```

### 6.5 Connection Management

```
Flask App â†’ Connection Pooling (PgBouncer) â†’ RDS PostgreSQL

PgBouncer in EKS:
- Pool mode: transaction
- Max connections: 100 (per app pod)
- DB max connections: 500 (RDS limit)
- Prevents "too many connections" errors at scale
```

### 6.6 Session Caching (ElastiCache Redis)

```
Flask API â†’ ElastiCache Redis (session/cache)

Configuration:
- Engine: Redis 7.x
- Node type: cache.t4g.micro â†’ cache.r7g.large (as needed)
- Multi-AZ with automatic failover
- Encryption at rest and in transit
- Use cases:
  - Session tokens
  - API response caching
  - Rate limiting counters
  - Background job queues
```

---

## 7. Security

### 7.1 Security Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   SECURITY LAYERS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L1: Edge       â”‚ CloudFront WAF, AWS Shield          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L2: Network    â”‚ VPC, Security Groups, NACLs,        â”‚
â”‚                â”‚ Private Subnets, Network Policies   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L3: Identity   â”‚ IAM least privilege, IRSA,          â”‚
â”‚                â”‚ MFA enforcement, AWS SSO             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L4: Data       â”‚ Encryption at rest (KMS),           â”‚
â”‚                â”‚ TLS in transit, Secrets Manager      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L5: App        â”‚ RBAC in Kubernetes, Pod Security    â”‚
â”‚                â”‚ Standards, non-root containers       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L6: Detection  â”‚ GuardDuty, CloudTrail, Falco,       â”‚
â”‚                â”‚ Security Hub, Config Rules           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 Key Security Controls

#### IAM & Access
- **Principle of least privilege** for all IAM roles
- **IRSA (IAM Roles for Service Accounts)** â€” no hardcoded credentials in pods
- **MFA required** for all human users
- **AWS SSO** for centralized access management
- **No long-lived access keys** â€” use IAM roles and instance profiles

#### Secrets Management
```
Kubernetes Secrets         â† External Secrets Operator â† AWS Secrets Manager
(used by pods)                  (syncs automatically)       (source of truth)

Database passwords, API keys, OAuth secrets â†’ stored ONLY in Secrets Manager
```

#### Kubernetes RBAC
```yaml
# Example: Developer RBAC - cannot see production secrets
kind: Role
metadata:
  name: developer
  namespace: production
rules:
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]    # Read-only in prod
- apiGroups: [""]
  resources: ["pods", "logs"]
  verbs: ["get", "list", "watch"]
# Explicitly NO access to secrets
```

#### Data Protection
- **RDS:** Encrypted at rest using AWS KMS, customer-managed key
- **S3:** Server-side encryption (SSE-S3 or SSE-KMS)
- **EBS volumes:** Encrypted at rest
- **Secrets Manager:** Automatic rotation for DB credentials (90-day cycle)
- **TLS everywhere:** Certificate Manager for free TLS certs

#### GDPR / Data Compliance Considerations
- Data residency: Single region deployment (EU region if needed)
- Data retention policies via S3 lifecycle rules
- Audit logging via CloudTrail (immutable, stored in Security account)
- Right to be forgotten: Application-level data deletion capability

---

## 8. CI/CD Pipeline

### 8.1 Pipeline Design

```
Developer Laptop
      â”‚
      â”‚ git push feature/new-feature
      â–¼
  GitHub Repository
      â”‚
      â”œâ”€â”€ Pull Request Created
      â”‚         â”‚
      â”‚         â–¼
      â”‚   GitHub Actions (PR checks)
      â”‚   â”œâ”€â”€ Run unit tests (pytest)
      â”‚   â”œâ”€â”€ Code linting (flake8, black)
      â”‚   â”œâ”€â”€ Security scan (Bandit, Trivy)
      â”‚   â””â”€â”€ Docker build test
      â”‚
      â”‚ PR Approved + Merged to main
      â”‚         â”‚
      â”‚         â–¼
      â”‚   GitHub Actions (CI Pipeline)
      â”‚   â”œâ”€â”€ Run all tests
      â”‚   â”œâ”€â”€ Build multi-arch Docker image
      â”‚   â”‚   (linux/amd64 + linux/arm64)
      â”‚   â”œâ”€â”€ Push to ECR
      â”‚   â”‚   â”œâ”€â”€ :latest
      â”‚   â”‚   â””â”€â”€ :<git-sha>
      â”‚   â””â”€â”€ Trigger deploy to Staging
      â”‚
      â”‚   Auto Deploy to Staging
      â”‚   â”œâ”€â”€ kubectl set image (or ArgoCD sync)
      â”‚   â”œâ”€â”€ Smoke tests
      â”‚   â””â”€â”€ Integration tests
      â”‚
      â”‚   Manual Approval Gate (Team Lead)
      â”‚         â”‚
      â”‚         â–¼
      â”‚   Deploy to Production
      â”‚   â”œâ”€â”€ Blue/Green or Rolling update
      â”‚   â”œâ”€â”€ Canary: 10% traffic â†’ 100%
      â”‚   â””â”€â”€ Automated rollback on errors
      â–¼
  Production Kubernetes Cluster
```

### 8.2 GitOps with ArgoCD (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GitOps Flow                       â”‚
â”‚                                                      â”‚
â”‚  App Repo          Config Repo (Helm Charts)         â”‚
â”‚  (code changes) â”€â–º (image tag updates)               â”‚
â”‚                            â”‚                         â”‚
â”‚                            â–¼                         â”‚
â”‚                       ArgoCD                         â”‚
â”‚                    (watches config repo)              â”‚
â”‚                            â”‚                         â”‚
â”‚                   Detects drift/changes              â”‚
â”‚                            â”‚                         â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚               â”‚                         â”‚            â”‚
â”‚               â–¼                         â–¼            â”‚
â”‚          Staging EKS              Production EKS     â”‚
â”‚          (auto-sync)              (manual approval)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 Deployment Strategy

| Phase | Strategy | Benefit |
|-------|----------|---------|
| **MVP** | Rolling update | Simple, zero-downtime |
| **Growth** | Canary (10% â†’ 100%) | Gradual rollout, catch issues early |
| **Scale** | Blue/Green | Instant switch, instant rollback |

---

## 9. Observability & Monitoring

### 9.1 Three Pillars of Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OBSERVABILITY STACK                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    METRICS     â”‚     LOGS       â”‚     TRACES        â”‚
â”‚                â”‚                â”‚                   â”‚
â”‚ CloudWatch     â”‚ CloudWatch     â”‚ AWS X-Ray         â”‚
â”‚ Container      â”‚ Logs           â”‚                   â”‚
â”‚ Insights       â”‚                â”‚ (distributed      â”‚
â”‚                â”‚ Fluent Bit â†’   â”‚  tracing for      â”‚
â”‚ Prometheus +   â”‚ CloudWatch     â”‚  Flask API)       â”‚
â”‚ Grafana        â”‚ Log Groups     â”‚                   â”‚
â”‚ (in-cluster)   â”‚                â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Key Metrics to Monitor

**Application Level:**
- Request latency (p50, p95, p99)
- Error rate (4xx, 5xx)
- Requests per second (RPS)
- Active users

**Infrastructure Level:**
- CPU / Memory utilization per pod/node
- Node provisioning time (Karpenter)
- Database connections, query latency
- Cache hit rate (Redis)

**Business Level:**
- User signups, active sessions
- API endpoint usage
- Cost per request

### 9.3 Alerting (CloudWatch Alarms)

```
Critical Alerts (Page on-call):
â”œâ”€â”€ API error rate > 5% for 5 minutes
â”œâ”€â”€ RDS CPU > 85% for 10 minutes
â”œâ”€â”€ Pod crash loop > 3 times
â””â”€â”€ EKS node unreachable

Warning Alerts (Slack notification):
â”œâ”€â”€ API error rate > 1% for 5 minutes
â”œâ”€â”€ RDS CPU > 60% for 15 minutes
â”œâ”€â”€ Memory utilization > 80%
â””â”€â”€ Karpenter failed to provision node
```

---

## 10. Cost Optimization

### 10.1 Cost Strategy

```
Cost Savings Strategy:

1. Spot Instances (60-90% savings)
   â””â”€â”€ All Karpenter-provisioned nodes use Spot first
       Fallback to On-Demand only if no Spot available

2. Graviton ARM64 (20% savings vs x86)
   â””â”€â”€ Flask API is Python â€” runs perfectly on ARM64
       Prefer Graviton NodePool for all stateless workloads

3. Karpenter Consolidation
   â””â”€â”€ Automatically removes unused nodes
       Bin-packs pods to minimize node count

4. RDS Right-sizing
   â””â”€â”€ Start: db.t3.medium (~$50/month)
       Monitor: scale up only when CPU/memory demand requires

5. S3 for Static Frontend
   â””â”€â”€ No EC2/EKS compute needed for React SPA
       Served via CloudFront (~$0.01/GB)
```

### 10.2 Estimated Monthly Cost (Low-Traffic Phase)

| Component | Service | Est. Cost/Month |
|-----------|---------|----------------|
| EKS Cluster Control Plane | EKS | $72 |
| System Nodes (2x t3.medium) | EC2 On-Demand | $60 |
| API Pods (Karpenter, Spot t4g.small) | EC2 Spot | ~$10-20 |
| RDS PostgreSQL (db.t3.medium, Multi-AZ) | RDS | ~$80 |
| ElastiCache Redis (cache.t4g.micro) | ElastiCache | ~$15 |
| ALB | ELB | ~$20 |
| CloudFront + S3 (SPA) | CloudFront + S3 | ~$5 |
| NAT Gateways (3x) | VPC | ~$100 |
| Data Transfer | Various | ~$10 |
| **Total (MVP)** | | **~$370/month** |

> **At scale (millions of users):** The architecture scales linearly â€” Karpenter adds nodes as needed, Spot pricing keeps costs manageable.

### 10.3 Cost Monitoring

- **AWS Cost Explorer** â€” daily cost trends, service breakdown
- **AWS Budgets** â€” alert when monthly spend > threshold
- **Kubecost** â€” per-namespace/per-team cost attribution in Kubernetes
- **Tagging strategy** â€” every resource tagged with `env`, `team`, `service`

---

## 11. Scalability Strategy

### 11.1 Scaling Layers

```
User Request Load Increases
        â”‚
        â–¼ Layer 1: CloudFront
   CDN absorbs static traffic (React SPA)
   Cache API responses where appropriate
        â”‚
        â–¼ Layer 2: Application Load Balancer
   Distributes traffic across healthy pods
        â”‚
        â–¼ Layer 3: HPA (Horizontal Pod Autoscaler)
   Adds more Flask API pods
   (scales in seconds)
        â”‚
        â–¼ Layer 4: Karpenter
   Provisions new EC2 nodes for the pods
   (scales in ~2 minutes)
        â”‚
        â–¼ Layer 5: RDS Read Replicas
   Add read replicas for read-heavy traffic
   (scales in ~10 minutes)
```

### 11.2 Scale-to-Zero for Non-Production

```
Development environment:
â”œâ”€â”€ EKS nodes: Karpenter scales to 0 at night
â”œâ”€â”€ RDS: Use db.t3.micro (single-AZ)
â”œâ”€â”€ Redis: cache.t4g.micro
â””â”€â”€ Schedule: Stop at 8pm, start at 8am
    (saves ~60% of dev environment costs)
```

### 11.3 Multi-Region (Future: Scale Phase)

```
Phase 3 (millions of users):

         Global Route 53 (Latency-based routing)
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚
    us-east-1              eu-west-1
   (primary)               (EMEA users)
         â”‚                     â”‚
       EKS                   EKS
       RDS Primary    â†â”€â”€â”€â”€ RDS Read Replica
                            (can promote if needed)
```

---

## 12. Disaster Recovery

### 12.1 DR Strategy: Warm Standby

```
Recovery Targets:
â”œâ”€â”€ RTO (Recovery Time Objective):  < 30 minutes
â””â”€â”€ RPO (Recovery Point Objective): < 5 minutes
```

### 12.2 DR Runbook

```
Scenario: Primary Region (us-east-1) complete failure

Step 1: Alert fires (CloudWatch/PagerDuty)          ~0 min
Step 2: On-call confirms region failure             ~5 min
Step 3: Promote RDS read replica in DR region       ~10 min
Step 4: Update Route 53 to point to DR region       ~2 min
Step 5: EKS cluster in DR region auto-scaled        ~5 min
Step 6: Smoke tests confirm DR region working       ~5 min
Step 7: Communicate to users (status page)          ~2 min
                                                 â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:                                          ~29 minutes
```

### 12.3 DR Architecture

```
PRIMARY REGION (us-east-1)
â”œâ”€â”€ EKS cluster (active)
â”œâ”€â”€ RDS PostgreSQL (primary)
â”œâ”€â”€ ElastiCache Redis (active)
â””â”€â”€ S3 buckets (active)
         â”‚
         â”‚  Replication
         â–¼
DISASTER RECOVERY REGION (us-west-2)
â”œâ”€â”€ EKS cluster (warm â€” scaled down, ready to scale up)
â”œâ”€â”€ RDS Read Replica (can promote to primary)
â”œâ”€â”€ ElastiCache Redis (warm)
â””â”€â”€ S3 buckets (cross-region replication enabled)
```

### 12.4 Backup Validation

- **Monthly DR drill** â€” actually failover to DR region, verify it works
- **Automated backup validation** â€” weekly Lambda function restores RDS snapshot to test instance
- **Recovery runbook** â€” documented, practiced, version-controlled in GitHub

---

## 13. Technology Decisions Summary

| Area | Choice | Justification |
|------|--------|---------------|
| **Cloud Provider** | AWS | Mature EKS, strong managed services ecosystem |
| **Kubernetes** | Amazon EKS | Managed control plane, AWS integrations |
| **Autoscaling** | Karpenter | Faster, more flexible than Cluster Autoscaler |
| **Container Registry** | Amazon ECR | Native EKS integration, image scanning, private |
| **Database** | RDS PostgreSQL â†’ Aurora | Start simple, scale when needed |
| **Cache** | ElastiCache Redis | Managed, HA, multi-AZ, session/cache |
| **CDN** | CloudFront | Global, WAF integration, free TLS |
| **SPA Hosting** | S3 + CloudFront | Zero server cost, global distribution |
| **Load Balancer** | ALB (EKS LB Controller) | Native K8s ingress, path-based routing |
| **Secrets** | AWS Secrets Manager | Centralized, rotation, audit trail |
| **DNS** | Route 53 | Private hosted zones, health checks |
| **Monitoring** | CloudWatch + Grafana | Native AWS + rich dashboards |
| **Tracing** | AWS X-Ray | Flask middleware available |
| **CI/CD** | GitHub Actions + ArgoCD | GitOps, declarative, audit trail |
| **IaC** | Terraform | Multi-provider, large ecosystem |

---

## ğŸ“š References

- [AWS EKS Best Practices Guide](https://aws.github.io/aws-eks-best-practices/)
- [Karpenter Documentation](https://karpenter.sh/)
- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [GDPR AWS Compliance](https://aws.amazon.com/compliance/gdpr-center/)
- [Kubernetes Security Best Practices](https://kubernetes.io/docs/concepts/security/)
- [12 Factor App Methodology](https://12factor.net/)

---

*Architecture designed for Innovate Inc. â€” February 2026*
*Reviewed against AWS Well-Architected Framework: Security, Reliability, Performance, Cost Optimization, Operational Excellence, Sustainability pillars.*
